{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.context import SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preferences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = SparkSession.builder.master(\"local\")\\\n",
    "                              .appName(\"project2\")\\\n",
    "                              .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema(column_names, required_columns_type):\n",
    "    \"\"\"\n",
    "    To get schema of the file.\n",
    "    \"\"\"\n",
    "    struct_field_list = [StructField(name, column_type, True)\n",
    "                         for name, column_type in zip(column_names, required_columns_type)]\n",
    "    return StructType(struct_field_list)\n",
    "\n",
    "\n",
    "def load_data(file_path, schema, delimiter):\n",
    "    return sparkSession.read.format(\"csv\").option(\"delimiter\", delimiter)\\\n",
    "                           .schema(schema)\\\n",
    "                           .load(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load HR.CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+----------------+----------+---------------+--------------------+--------------+--------------+\n",
      "|EmployeeID|ManagerID|EmployeeFirstName|EmployeeLastName|EmployeeMI|EmployeeJobCode|      EmployeeBranch|EmployeeOffice| EmployeePhone|\n",
      "+----------+---------+-----------------+----------------+----------+---------------+--------------------+--------------+--------------+\n",
      "|         0|      702|            Ozkan|         Douglas|      null|            647|EGZKSobTeknHCbLuH...|    OFFICE7152|(726) 088-3331|\n",
      "|         1|     1377|             Suer|         Candice|      null|            314|OfOBVvpzNvHCebxyu...|    OFFICE8586|(344) 999-2652|\n",
      "|         2|      819|        Somisetty|            Jami|         P|            534|rAHWYkktOXAyPAYHl...|          null|(984) 538-5366|\n",
      "|         3|      824|          Mazurek|       Rosalinda|         J|            364|TJQqsUQQGqWG QleL...|    OFFICE8487|(860) 037-6897|\n",
      "|         4|     4345|        Aronovich|        Delphine|         M|            314|IEMJHuQgCPDHCwwJk...|    OFFICE9420|(604) 387-9350|\n",
      "+----------+---------+-----------------+----------------+----------+---------------+--------------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_names = [\"EmployeeID\", \"ManagerID\", \"EmployeeFirstName\", \"EmployeeLastName\",\n",
    "               \"EmployeeMI\", \"EmployeeJobCode\", \"EmployeeBranch\", \"EmployeeOffice\",\n",
    "               \"EmployeePhone\"]\n",
    "required_columns_type = [StringType(), StringType(), StringType(), StringType(),\n",
    "                        StringType(), StringType(), StringType(), StringType(),\n",
    "                        StringType(),BooleanType(), IntegerType(), DateType(), DateType()]\n",
    "file_path= \"Dataset/Batch1/HR.csv\"\n",
    "schema = get_schema(column_names, required_columns_type)\n",
    "hr_df = load_data(file_path, schema, \",\")\n",
    "hr_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Date.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------------+--------------+----------------+-------------+---------------+---------------+-----------------+--------------+----------------+------------+-------------+------------+--------------+-----------+-------------+-----------+\n",
      "|SK_DateID| DateValue|       DateDesc|CalendarYearID|CalendarYearDesc|CalendarQtrID|CalendarQtrDesc|CalendarMonthID|CalendarMonthDesc|CalendarWeekID|CalendarWeekDesc|DayOfWeekNum|DayOfWeekDesc|FiscalYearID|FiscalYearDesc|FiscalQtrID|FiscalQtrDesc|HolidayFlag|\n",
      "+---------+----------+---------------+--------------+----------------+-------------+---------------+---------------+-----------------+--------------+----------------+------------+-------------+------------+--------------+-----------+-------------+-----------+\n",
      "| 19500101|1950-01-01|January 1, 1950|          1950|            1950|        19501|        1950 Q1|          19501|     1950 January|         19501|         1950-W1|           7|       Sunday|        1950|          1950|      19503|      1950 Q3|       true|\n",
      "| 19500102|1950-01-02|January 2, 1950|          1950|            1950|        19501|        1950 Q1|          19501|     1950 January|         19501|         1950-W1|           1|       Monday|        1950|          1950|      19503|      1950 Q3|      false|\n",
      "| 19500103|1950-01-03|January 3, 1950|          1950|            1950|        19501|        1950 Q1|          19501|     1950 January|         19501|         1950-W1|           2|      Tuesday|        1950|          1950|      19503|      1950 Q3|      false|\n",
      "| 19500104|1950-01-04|January 4, 1950|          1950|            1950|        19501|        1950 Q1|          19501|     1950 January|         19501|         1950-W1|           3|    Wednesday|        1950|          1950|      19503|      1950 Q3|      false|\n",
      "| 19500105|1950-01-05|January 5, 1950|          1950|            1950|        19501|        1950 Q1|          19501|     1950 January|         19501|         1950-W1|           4|     Thursday|        1950|          1950|      19503|      1950 Q3|      false|\n",
      "+---------+----------+---------------+--------------+----------------+-------------+---------------+---------------+-----------------+--------------+----------------+------------+-------------+------------+--------------+-----------+-------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Date\n",
    "column_names = [\"SK_DateID\", \"DateValue\", \"DateDesc\", \"CalendarYearID\",\n",
    "               \"CalendarYearDesc\", \"CalendarQtrID\", \"CalendarQtrDesc\", \"CalendarMonthID\",\n",
    "               \"CalendarMonthDesc\", \"CalendarWeekID\", \"CalendarWeekDesc\", \"DayOfWeekNum\",\n",
    "               \"DayOfWeekDesc\", \"FiscalYearID\", \"FiscalYearDesc\", \"FiscalQtrID\", \n",
    "               \"FiscalQtrDesc\", \"HolidayFlag\"]\n",
    "required_columns_type = [IntegerType(), DateType(), StringType(), IntegerType(), StringType(),\n",
    "                        IntegerType(), StringType(), IntegerType(), StringType(), IntegerType(),\n",
    "                        StringType(), IntegerType(), StringType(), IntegerType(), \n",
    "                        StringType(), IntegerType(), StringType(), BooleanType()]\n",
    "\n",
    "file_path= \"Dataset/Batch1/Date.txt\"\n",
    "schema = get_schema(column_names, required_columns_type)\n",
    "date_df = load_data(file_path, schema, \"|\")\n",
    "date_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Time.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------+--------+--------+----------+--------+----------+---------------+---------------+\n",
      "|SK_TimeID|TimeValue|HourID|HourDesc|MinuteID|MinuteDesc|SecondID|SecondDesc|MarketHoursFlag|OfficeHoursFlag|\n",
      "+---------+---------+------+--------+--------+----------+--------+----------+---------------+---------------+\n",
      "|        0| 00:00:00|     0|      00|       0|     00:00|       0|  00:00:00|          false|          false|\n",
      "|        1| 00:00:01|     0|      00|       0|     00:00|       1|  00:00:01|          false|          false|\n",
      "|        2| 00:00:02|     0|      00|       0|     00:00|       2|  00:00:02|          false|          false|\n",
      "|        3| 00:00:03|     0|      00|       0|     00:00|       3|  00:00:03|          false|          false|\n",
      "|        4| 00:00:04|     0|      00|       0|     00:00|       4|  00:00:04|          false|          false|\n",
      "+---------+---------+------+--------+--------+----------+--------+----------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Time\n",
    "column_names = [\"SK_TimeID\", \"TimeValue\", \"HourID\", \"HourDesc\",\n",
    "               \"MinuteID\", \"MinuteDesc\", \"SecondID\", \"SecondDesc\",\n",
    "               \"MarketHoursFlag\", \"OfficeHoursFlag\"]\n",
    "required_columns_type = [IntegerType(),StringType(), IntegerType(),\n",
    "                        StringType(), IntegerType(), StringType(), IntegerType(), \n",
    "                        StringType(), BooleanType(), BooleanType()]\n",
    "file_path= \"Dataset/Batch1/Time.txt\"\n",
    "schema = get_schema(column_names, required_columns_type)\n",
    "time_df = load_data(file_path, schema, \"|\")\n",
    "time_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load CashTransaction.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------+--------+--------+----------+--------+----------+---------------+---------------+\n",
      "|SK_TimeID|TimeValue|HourID|HourDesc|MinuteID|MinuteDesc|SecondID|SecondDesc|MarketHoursFlag|OfficeHoursFlag|\n",
      "+---------+---------+------+--------+--------+----------+--------+----------+---------------+---------------+\n",
      "|   000000| 00:00:00|    00|      00|      00|     00:00|      00|  00:00:00|          false|          false|\n",
      "|   000001| 00:00:01|    00|      00|      00|     00:00|      01|  00:00:01|          false|          false|\n",
      "|   000002| 00:00:02|    00|      00|      00|     00:00|      02|  00:00:02|          false|          false|\n",
      "|   000003| 00:00:03|    00|      00|      00|     00:00|      03|  00:00:03|          false|          false|\n",
      "|   000004| 00:00:04|    00|      00|      00|     00:00|      04|  00:00:04|          false|          false|\n",
      "+---------+---------+------+--------+--------+----------+--------+----------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load CashTransaction\n",
    "column_names = [\"SK_TimeID\", \"TimeValue\", \"HourID\", \"HourDesc\",\n",
    "               \"MinuteID\", \"MinuteDesc\", \"SecondID\", \"SecondDesc\",\n",
    "               \"MarketHoursFlag\", \"OfficeHoursFlag\"]\n",
    "file_path= \"Dataset/Batch1/Time.txt\"\n",
    "schema = get_schema(column_names)\n",
    "date_df = load_data(file_path, schema, \"|\")\n",
    "date_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load StatusType.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|ST_ID|  ST_NAME|\n",
      "+-----+---------+\n",
      "| ACTV|   Active|\n",
      "| CMPT|Completed|\n",
      "| CNCL| Canceled|\n",
      "| PNDG|  Pending|\n",
      "| SBMT|Submitted|\n",
      "+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_names= [\"ST_ID\", \"ST_NAME\"]\n",
    "required_columns_type = [StringType(), StringType()]\n",
    "file_path= \"Dataset/Batch1/StatusType.txt\"\n",
    "schema = get_schema(column_names, required_columns_type)\n",
    "status_df = load_data(file_path, schema, \"|\")\n",
    "date_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Create DimDate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The source file of this dimention is: Date.txt.**\n",
    "    \n",
    "    -- We have already load Date.txt file as dataframe with required types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DimDate = date_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Create DimTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The source file of this dimention is: Date.txt.**\n",
    "    \n",
    "    -- We have already load time.txt file as dataframe with required types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DimTime = time_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Create DimBroker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The source file of this dimention is: HR.txt.**\n",
    "    \n",
    "    -- We have already load HR.txt file as dataframe with required types. But we need to filter employees with code = 314"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+----------------+----------+---------------+--------------------+--------------+--------------+\n",
      "|EmployeeID|ManagerID|EmployeeFirstName|EmployeeLastName|EmployeeMI|EmployeeJobCode|      EmployeeBranch|EmployeeOffice| EmployeePhone|\n",
      "+----------+---------+-----------------+----------------+----------+---------------+--------------------+--------------+--------------+\n",
      "|         1|     1377|             Suer|         Candice|      null|            314|OfOBVvpzNvHCebxyu...|    OFFICE8586|(344) 999-2652|\n",
      "|         4|     4345|        Aronovich|        Delphine|         M|            314|IEMJHuQgCPDHCwwJk...|    OFFICE9420|(604) 387-9350|\n",
      "|         8|     2146|           Hansen|        Montreal|         T|            314|sGIpORbLsRjTdhqBN...|    OFFICE6343|(991) 491-4907|\n",
      "|        11|     2259|       Charchanko|          Sheela|      null|            314|Cw QJMHPgpozCKsFZ...|    OFFICE7705|(977) 726-0106|\n",
      "|        14|     3663|            Knorp|            Uday|      null|            314|QmCLAAAJibegHoPZc...|    OFFICE6437|(254) 560-8156|\n",
      "+----------+---------+-----------------+----------------+----------+---------------+--------------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DimBroker = hr_df.filter(hr_df.EmployeeJobCode == 314)\n",
    "DimBroker.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Create DimStatusType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "DimStatusType = date_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Create DimAccount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get action types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldoc = minidom.parse('Dataset/CustomerMgmt.xml')\n",
    "itemlist = xmldoc.getElementsByTagName('@TPCDI:Action')\n",
    "\n",
    "# Get actions types.\n",
    "action_types = []\n",
    "for item in itemlist:\n",
    "    action_types.append(action_type)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## الشغل الي تحت عك "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load status table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's join Account table with status table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_with_status = account_df.join(\n",
    "                        broadcast(status_df), \n",
    "                        account_df.CA_ST_ID == status_df.ST_ID,   \n",
    "                        'inner'\n",
    "                  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----+--------+--------+--------------------+---------+--------+-----+--------+\n",
      "|CDC_FLAG|CDC_DSN|CA_ID|CA_B_ID |CA_C_ID |             CA_NAME|CA_TAX_ST|CA_ST_ID|ST_ID| ST_NAME|\n",
      "+--------+-------+-----+--------+--------+--------------------+---------+--------+-----+--------+\n",
      "|       I|  43490|30470|   16206|   15280|XkRcJWPVFFSGAtTGo...|        1|    ACTV| ACTV|  Active|\n",
      "|       U|  43491|13857|   35351|    4996|kXUQTTuZHQsJsIDcB...|        1|    ACTV| ACTV|  Active|\n",
      "|       U|  43492|26685|   23304|    2762|ruXPPxRMDLjswZZHv...|        1|    INAC| INAC|Inactive|\n",
      "|       I|  43493|30471|   43026|   15281|arQHNWBBCOGMxvWqT...|        2|    ACTV| ACTV|  Active|\n",
      "|       I|  43494|30472|    5711|   15282|DuQgzgldMMnEnh Fh...|        1|    ACTV| ACTV|  Active|\n",
      "+--------+-------+-----+--------+--------+--------------------+---------+--------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "account_with_status.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load customer.xml file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a generator of dict containing account data \n",
    "accounts_dict = ET.parse('Dataset/CustomerMgmt.xml').iter(\"@ActionType\")    \n",
    "c_ids = []\n",
    "for x in accounts_dict:\n",
    "    print(x)\n",
    "    if x.find(\"Account\") != None:\n",
    "        c_ids.append(x.attrib[\"C_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldoc = minidom.parse('Dataset/CustomerMgmt.xml')\n",
    "itemlist = xmldoc.getElementsByTagName('@TPCDI:Action')\n",
    "\n",
    "# Get actions types.\n",
    "action_types = []\n",
    "# Get surrogent keys of customers.\n",
    "ca_ids = []\n",
    "\n",
    "for item in itemlist:\n",
    "    action_type = item.attributes[\"ActionType\"].value\n",
    "    if action_type != \"INACT\" and item.getElementsByTagName(\"Account\"):\n",
    "        ca_ids.append(item.getElementsByTagName(\"Account\")[0].attributes[\"CA_ID\"].value)\n",
    "        action_types.append(action_type)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data_array = []\n",
    "for x, y, z in zip(c_ids, action_types, ca_ids):\n",
    "    customer_data_array.append([x, y, z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Dataframe \n",
    "customer_pd = pd.DataFrame(customer_data_array, \n",
    "                 columns=[\"C_ID\", \"ACTION_TYPE\", \"CA_ID\"], index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----+\n",
      "|C_ID|ACTION_TYPE|CA_ID|\n",
      "+----+-----------+-----+\n",
      "|   0|        NEW|    0|\n",
      "|   1|        NEW|    1|\n",
      "|   2|        NEW|    2|\n",
      "|   3|        NEW|    3|\n",
      "|   4|        NEW|    4|\n",
      "+----+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mySchema = StructType([ StructField(\"C_ID\", StringType(), True)\\\n",
    "\n",
    "                       ,StructField(\"ACTION_TYPE\", StringType(), True)\\\n",
    "\n",
    "                       ,StructField(\"CA_ID\", StringType(), True)])\n",
    "customer_df = sparkSession.createDataFrame(customer_pd, schema = mySchema)\n",
    "\n",
    "customer_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join account_with_status with customer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_account_df = account_with_status.join(\n",
    "                        customer_df, \n",
    "                        account_with_status.CA_ID == customer_df.CA_ID,   \n",
    "                        'inner'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----+--------+--------+--------------------+---------+--------+-----+--------+----+-----------+-----+\n",
      "|CDC_FLAG|CDC_DSN|CA_ID|CA_B_ID |CA_C_ID |             CA_NAME|CA_TAX_ST|CA_ST_ID|ST_ID| ST_NAME|C_ID|ACTION_TYPE|CA_ID|\n",
      "+--------+-------+-----+--------+--------+--------------------+---------+--------+-----+--------+----+-----------+-----+\n",
      "|       U|  43579|  751|   36480|     436|SBkXmBJLgAbOmSROj...|        1|    INAC| INAC|Inactive| 436|        NEW|  751|\n",
      "|       U|  43574| 1143|    1474|     618|INkSQXOCuakseRkSa...|        2|    ACTV| ACTV|  Active| 618|        NEW| 1143|\n",
      "|       U|  43561| 3568|   30347|    1761|LgEiiaOJQMRJNcDMm...|        1|    ACTV| ACTV|  Active|1761|    ADDACCT| 3568|\n",
      "|       U|  43561| 3568|   30347|    1761|LgEiiaOJQMRJNcDMm...|        1|    ACTV| ACTV|  Active|1761|    UPDACCT| 3568|\n",
      "|       U|  43553| 4128|   28792|    1607|FdnzvlBxEzFnsRpVd...|        1|    INAC| INAC|Inactive|1607|    ADDACCT| 4128|\n",
      "+--------+-------+-----+--------+--------+--------------------+---------+--------+-----+--------+----+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_account_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['CA_ST_ID', 'ST_ID', 'CA_B_ID', 'CA_ID', 'CA_B_ID']\n",
    "dim_account_df = dim_account_df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+--------+--------------------+---------+--------+----+-----------+\n",
      "|CDC_FLAG|CDC_DSN|CA_B_ID |CA_C_ID |             CA_NAME|CA_TAX_ST| ST_NAME|C_ID|ACTION_TYPE|\n",
      "+--------+-------+--------+--------+--------------------+---------+--------+----+-----------+\n",
      "|       U|  43579|   36480|     436|SBkXmBJLgAbOmSROj...|        1|Inactive| 436|        NEW|\n",
      "|       U|  43574|    1474|     618|INkSQXOCuakseRkSa...|        2|  Active| 618|        NEW|\n",
      "|       U|  43561|   30347|    1761|LgEiiaOJQMRJNcDMm...|        1|  Active|1761|    ADDACCT|\n",
      "|       U|  43561|   30347|    1761|LgEiiaOJQMRJNcDMm...|        1|  Active|1761|    UPDACCT|\n",
      "|       U|  43553|   28792|    1607|FdnzvlBxEzFnsRpVd...|        1|Inactive|1607|    ADDACCT|\n",
      "+--------+-------+--------+--------+--------------------+---------+--------+----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_account_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CDC_FLAG',\n",
       " 'CDC_DSN',\n",
       " 'CA_B_ID ',\n",
       " 'CA_C_ID ',\n",
       " 'CA_NAME',\n",
       " 'CA_TAX_ST',\n",
       " 'ST_NAME',\n",
       " 'C_ID',\n",
       " 'ACTION_TYPE']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_account_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate the dim table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-1b3c7e69ca9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHiveContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msqlContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHiveContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "sqlContext = HiveContext(sc)\n",
    "\n",
    "sqlContext.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\")\n",
    "sqlContext.sql(\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\")\n",
    "\n",
    "# Queries can be expressed in HiveQL.\n",
    "results = sqlContext.sql(\"FROM src SELECT key, value\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BinaryType – Binary data.\n",
    "BooleanType – Boolean values.\n",
    "ByteType – A byte value.\n",
    "DateType – A datetime value.\n",
    "DoubleType – A floating-point double value.\n",
    "IntegerType – An integer value.\n",
    "LongType – A long integer value.\n",
    "NullType – A null value.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
