{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast, current_timestamp\n",
    "from pyspark.sql.context import SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preferences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = SparkSession.builder.master(\"local\")\\\n",
    "                              .appName(\"project2\")\\\n",
    "                              .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema(column_names, required_columns_type):\n",
    "    \"\"\"\n",
    "    To get schema of the file.\n",
    "    \"\"\"\n",
    "    struct_field_list = [StructField(name, column_type, True)\n",
    "                         for name, column_type in zip(column_names, required_columns_type)]\n",
    "    return StructType(struct_field_list)\n",
    "\n",
    "\n",
    "def load_data(file_path, schema, delimiter):\n",
    "    return sparkSession.read.format(\"csv\").option(\"delimiter\", delimiter)\\\n",
    "                           .schema(schema)\\\n",
    "                           .load(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load HR.CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+----------------+----------+---------------+--------------------+--------------+--------------+\n",
      "|EmployeeID|ManagerID|EmployeeFirstName|EmployeeLastName|EmployeeMI|EmployeeJobCode|      EmployeeBranch|EmployeeOffice| EmployeePhone|\n",
      "+----------+---------+-----------------+----------------+----------+---------------+--------------------+--------------+--------------+\n",
      "|         0|      702|            Ozkan|         Douglas|      null|            647|EGZKSobTeknHCbLuH...|    OFFICE7152|(726) 088-3331|\n",
      "|         1|     1377|             Suer|         Candice|      null|            314|OfOBVvpzNvHCebxyu...|    OFFICE8586|(344) 999-2652|\n",
      "|         2|      819|        Somisetty|            Jami|         P|            534|rAHWYkktOXAyPAYHl...|          null|(984) 538-5366|\n",
      "|         3|      824|          Mazurek|       Rosalinda|         J|            364|TJQqsUQQGqWG QleL...|    OFFICE8487|(860) 037-6897|\n",
      "|         4|     4345|        Aronovich|        Delphine|         M|            314|IEMJHuQgCPDHCwwJk...|    OFFICE9420|(604) 387-9350|\n",
      "+----------+---------+-----------------+----------------+----------+---------------+--------------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_names = [\"EmployeeID\", \"ManagerID\", \"EmployeeFirstName\", \"EmployeeLastName\",\n",
    "               \"EmployeeMI\", \"EmployeeJobCode\", \"EmployeeBranch\", \"EmployeeOffice\",\n",
    "               \"EmployeePhone\"]\n",
    "required_columns_type = [StringType(), StringType(), StringType(), StringType(),\n",
    "                        StringType(), StringType(), StringType(), StringType(),\n",
    "                        StringType(),BooleanType(), IntegerType(), DateType(), DateType()]\n",
    "file_path= \"Dataset/Batch1/HR.csv\"\n",
    "schema = get_schema(column_names, required_columns_type)\n",
    "hr_df = load_data(file_path, schema, \",\")\n",
    "hr_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Date.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------------+--------------+----------------+-------------+---------------+---------------+-----------------+--------------+----------------+------------+-------------+------------+--------------+-----------+-------------+-----------+\n",
      "|SK_DateID| DateValue|       DateDesc|CalendarYearID|CalendarYearDesc|CalendarQtrID|CalendarQtrDesc|CalendarMonthID|CalendarMonthDesc|CalendarWeekID|CalendarWeekDesc|DayOfWeekNum|DayOfWeekDesc|FiscalYearID|FiscalYearDesc|FiscalQtrID|FiscalQtrDesc|HolidayFlag|\n",
      "+---------+----------+---------------+--------------+----------------+-------------+---------------+---------------+-----------------+--------------+----------------+------------+-------------+------------+--------------+-----------+-------------+-----------+\n",
      "| 19500101|1950-01-01|January 1, 1950|          1950|            1950|        19501|        1950 Q1|          19501|     1950 January|         19501|         1950-W1|           7|       Sunday|        1950|          1950|      19503|      1950 Q3|       true|\n",
      "| 19500102|1950-01-02|January 2, 1950|          1950|            1950|        19501|        1950 Q1|          19501|     1950 January|         19501|         1950-W1|           1|       Monday|        1950|          1950|      19503|      1950 Q3|      false|\n",
      "| 19500103|1950-01-03|January 3, 1950|          1950|            1950|        19501|        1950 Q1|          19501|     1950 January|         19501|         1950-W1|           2|      Tuesday|        1950|          1950|      19503|      1950 Q3|      false|\n",
      "| 19500104|1950-01-04|January 4, 1950|          1950|            1950|        19501|        1950 Q1|          19501|     1950 January|         19501|         1950-W1|           3|    Wednesday|        1950|          1950|      19503|      1950 Q3|      false|\n",
      "| 19500105|1950-01-05|January 5, 1950|          1950|            1950|        19501|        1950 Q1|          19501|     1950 January|         19501|         1950-W1|           4|     Thursday|        1950|          1950|      19503|      1950 Q3|      false|\n",
      "+---------+----------+---------------+--------------+----------------+-------------+---------------+---------------+-----------------+--------------+----------------+------------+-------------+------------+--------------+-----------+-------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Date\n",
    "column_names = [\"SK_DateID\", \"DateValue\", \"DateDesc\", \"CalendarYearID\",\n",
    "               \"CalendarYearDesc\", \"CalendarQtrID\", \"CalendarQtrDesc\", \"CalendarMonthID\",\n",
    "               \"CalendarMonthDesc\", \"CalendarWeekID\", \"CalendarWeekDesc\", \"DayOfWeekNum\",\n",
    "               \"DayOfWeekDesc\", \"FiscalYearID\", \"FiscalYearDesc\", \"FiscalQtrID\", \n",
    "               \"FiscalQtrDesc\", \"HolidayFlag\"]\n",
    "required_columns_type = [IntegerType(), DateType(), StringType(), IntegerType(), StringType(),\n",
    "                        IntegerType(), StringType(), IntegerType(), StringType(), IntegerType(),\n",
    "                        StringType(), IntegerType(), StringType(), IntegerType(), \n",
    "                        StringType(), IntegerType(), StringType(), BooleanType()]\n",
    "\n",
    "file_path= \"Dataset/Batch1/Date.txt\"\n",
    "schema = get_schema(column_names, required_columns_type)\n",
    "date_df = load_data(file_path, schema, \"|\")\n",
    "date_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Time.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------+--------+--------+----------+--------+----------+---------------+---------------+\n",
      "|SK_TimeID|TimeValue|HourID|HourDesc|MinuteID|MinuteDesc|SecondID|SecondDesc|MarketHoursFlag|OfficeHoursFlag|\n",
      "+---------+---------+------+--------+--------+----------+--------+----------+---------------+---------------+\n",
      "|        0| 00:00:00|     0|      00|       0|     00:00|       0|  00:00:00|          false|          false|\n",
      "|        1| 00:00:01|     0|      00|       0|     00:00|       1|  00:00:01|          false|          false|\n",
      "|        2| 00:00:02|     0|      00|       0|     00:00|       2|  00:00:02|          false|          false|\n",
      "|        3| 00:00:03|     0|      00|       0|     00:00|       3|  00:00:03|          false|          false|\n",
      "|        4| 00:00:04|     0|      00|       0|     00:00|       4|  00:00:04|          false|          false|\n",
      "+---------+---------+------+--------+--------+----------+--------+----------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Time\n",
    "column_names = [\"SK_TimeID\", \"TimeValue\", \"HourID\", \"HourDesc\",\n",
    "               \"MinuteID\", \"MinuteDesc\", \"SecondID\", \"SecondDesc\",\n",
    "               \"MarketHoursFlag\", \"OfficeHoursFlag\"]\n",
    "required_columns_type = [IntegerType(),StringType(), IntegerType(),\n",
    "                        StringType(), IntegerType(), StringType(), IntegerType(), \n",
    "                        StringType(), BooleanType(), BooleanType()]\n",
    "file_path= \"Dataset/Batch1/Time.txt\"\n",
    "schema = get_schema(column_names, required_columns_type)\n",
    "time_df = load_data(file_path, schema, \"|\")\n",
    "time_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load CashTransaction.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------+--------+--------+----------+--------+----------+---------------+---------------+\n",
      "|SK_TimeID|TimeValue|HourID|HourDesc|MinuteID|MinuteDesc|SecondID|SecondDesc|MarketHoursFlag|OfficeHoursFlag|\n",
      "+---------+---------+------+--------+--------+----------+--------+----------+---------------+---------------+\n",
      "|   000000| 00:00:00|    00|      00|      00|     00:00|      00|  00:00:00|          false|          false|\n",
      "|   000001| 00:00:01|    00|      00|      00|     00:00|      01|  00:00:01|          false|          false|\n",
      "|   000002| 00:00:02|    00|      00|      00|     00:00|      02|  00:00:02|          false|          false|\n",
      "|   000003| 00:00:03|    00|      00|      00|     00:00|      03|  00:00:03|          false|          false|\n",
      "|   000004| 00:00:04|    00|      00|      00|     00:00|      04|  00:00:04|          false|          false|\n",
      "+---------+---------+------+--------+--------+----------+--------+----------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load CashTransaction\n",
    "column_names = [\"SK_TimeID\", \"TimeValue\", \"HourID\", \"HourDesc\",\n",
    "               \"MinuteID\", \"MinuteDesc\", \"SecondID\", \"SecondDesc\",\n",
    "               \"MarketHoursFlag\", \"OfficeHoursFlag\"]\n",
    "file_path= \"Dataset/Batch1/Time.txt\"\n",
    "schema = get_schema(column_names)\n",
    "date_df = load_data(file_path, schema, \"|\")\n",
    "date_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load StatusType.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|ST_ID|  ST_NAME|\n",
      "+-----+---------+\n",
      "| ACTV|   Active|\n",
      "| CMPT|Completed|\n",
      "| CNCL| Canceled|\n",
      "| PNDG|  Pending|\n",
      "| SBMT|Submitted|\n",
      "+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_names= [\"ST_ID\", \"ST_NAME\"]\n",
    "required_columns_type = [StringType(), StringType()]\n",
    "file_path= \"Dataset/Batch1/StatusType.txt\"\n",
    "schema = get_schema(column_names, required_columns_type)\n",
    "status_df = load_data(file_path, schema, \"|\")\n",
    "date_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Create DimDate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The source file of this dimention is: Date.txt.**\n",
    "    \n",
    "    -- We have already load Date.txt file as dataframe with required types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DimDate = date_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Create DimTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The source file of this dimention is: Date.txt.**\n",
    "    \n",
    "    -- We have already load time.txt file as dataframe with required types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DimTime = time_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Create DimBroker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The source file of this dimention is: HR.txt.**\n",
    "    \n",
    "    -- We have already load HR.txt file as dataframe with required types. But we need to filter employees with code = 314"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+----------------+----------+---------------+--------------------+--------------+--------------+\n",
      "|EmployeeID|ManagerID|EmployeeFirstName|EmployeeLastName|EmployeeMI|EmployeeJobCode|      EmployeeBranch|EmployeeOffice| EmployeePhone|\n",
      "+----------+---------+-----------------+----------------+----------+---------------+--------------------+--------------+--------------+\n",
      "|         1|     1377|             Suer|         Candice|      null|            314|OfOBVvpzNvHCebxyu...|    OFFICE8586|(344) 999-2652|\n",
      "|         4|     4345|        Aronovich|        Delphine|         M|            314|IEMJHuQgCPDHCwwJk...|    OFFICE9420|(604) 387-9350|\n",
      "|         8|     2146|           Hansen|        Montreal|         T|            314|sGIpORbLsRjTdhqBN...|    OFFICE6343|(991) 491-4907|\n",
      "|        11|     2259|       Charchanko|          Sheela|      null|            314|Cw QJMHPgpozCKsFZ...|    OFFICE7705|(977) 726-0106|\n",
      "|        14|     3663|            Knorp|            Uday|      null|            314|QmCLAAAJibegHoPZc...|    OFFICE6437|(254) 560-8156|\n",
      "+----------+---------+-----------------+----------------+----------+---------------+--------------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DimBroker = hr_df.filter(hr_df.EmployeeJobCode == 314)\n",
    "DimBroker.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Create DimStatusType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "DimStatusType = date_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Create DimAccount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "    -- we use 'minidom' beacuse ET can't see tag <TPCDI:Action>.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of actions in customer file is 50000.\n"
     ]
    }
   ],
   "source": [
    "xmldoc = minidom.parse('Dataset/CustomerMgmt.xml')\n",
    "action_tag_for_customers = xmldoc.getElementsByTagName('TPCDI:Action')\n",
    "\n",
    "# Get actions types.\n",
    "action_type_for_customers = [item.attributes[\"ActionType\"].value \n",
    "                             for item in action_tag_for_customers]\n",
    "print(\"The number of actions in customer file is {}.\".format(len(action_type_for_customers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name_for_dim_account = [\"SK_AccountID\", \"AccountID\", \"SK_BrokerID\", \"SK_CustomerID\", \"Status\", \n",
    "                \"AccountDesc\", \"TaxStatus\", \"IsCurrent\", \"BatchID\", \"EffectiveDate\", \n",
    "                \"EndDate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use if condition to handle the actiontype cases \n",
    "# 1 - [\"NEW\", \"ADDACCT\"] all data exist\n",
    "# 2 - UPDACCT only updated part exist.\n",
    "# 3 - Other: the account is closed or deactvated.\n",
    "\n",
    "accounts = ET.parse('Dataset/CustomerMgmt.xml').iter(\"Customer\")    \n",
    "\n",
    "Dim_account_list= []\n",
    "sk = 0\n",
    "for i, x in enumerate(accounts, 0):\n",
    "    \n",
    "    if action_type_for_customers[i] in [\"NEW\", \"ADDACCT\"]:\n",
    "        # Here: We extract all data we need from xml \n",
    "        C_ID = x.attrib[\"C_ID\"]\n",
    "        CA_ID = x.find(\"Account\").attrib[\"CA_ID\"]\n",
    "        CA_TAX_ST = x.find(\"Account\").attrib[\"CA_TAX_ST\"]\n",
    "        CA_B_ID = x.find(\"Account/CA_B_ID\").text\n",
    "        CA_NAME = x.find(\"Account/CA_NAME\").text\n",
    "        Dim_account_list.append((sk, CA_ID, CA_B_ID, C_ID, \"Active\", CA_NAME, CA_TAX_ST, True, \n",
    "               1, datetime.now(), \"9999-12-31\"))\n",
    "        \n",
    "    elif action_type_for_customers[i] == \"UPDACCT\":\n",
    "        C_ID = x.attrib[\"C_ID\"]\n",
    "        CA_ID = x.find(\"Account\").attrib[\"CA_ID\"]\n",
    "        CA_TAX_ST = x.find(\"Account\").attrib[\"CA_TAX_ST\"] if \"CA_TAX_ST\" in x.find(\"Account\").attrib else None\n",
    "        \n",
    "        # check if part is exist if not set it to None.\n",
    "        CA_B_ID = x.find(\"Account/CA_B_ID\").text if x.find(\"Account/CA_B_ID\") != None else None\n",
    "        CA_NAME = x.find(\"Account/CA_NAME\").text if x.find(\"Account/CA_NAME\") != None else None\n",
    "        new_values = (sk, CA_ID, CA_B_ID, C_ID, \"Active\", CA_NAME, CA_TAX_ST, True, \n",
    "               1, datetime.now(), \"9999-12-31\")\n",
    "        \n",
    "        # Get the last record with selected CA_ID.\n",
    "        last_account = DimAccount_new_addacc.filter(DimAccount_new_addacc.AccountID == CA_ID).collect()[-1]  \n",
    "        \n",
    "        # Get the upaded value if exist if not set it to old value \n",
    "        updated_account = tuple([update if update != None else last \n",
    "                                     for update, last in zip(new_values, last_account)])  \n",
    "        \n",
    "        # Append it \n",
    "        Dim_account_list.append(updated_account)\n",
    "    elif action_type_for_customers[i] in [\"CLOSEACCT\", \"INACT\"]:\n",
    "        # The account is colosed or inactive\n",
    "        last_account = list(DimAccount_new_addacc.filter(DimAccount_new_addacc.AccountID == CA_ID).collect()[-1]) \n",
    "        last_account[5] = \"closed\"\n",
    "        Dim_account_list.append(tuple(last_account))\n",
    "    sk+=1\n",
    "\n",
    "DimAccount = sparkSession.createDataFrame(Dim_account_list, columns_name)\n",
    "DimAccount.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save this dim as it takes a lot of time to process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_column_name= [\"SK_CustomerID\", \"CustomerID\", \"TaxID\", \"Status\", \"LastName\", \"FirstName\",\n",
    "                      \"MiddleInitial\", \"Gender\", \"Tier\", \"DOB\", \"AddressLine1\", \"AddressLine2\",\n",
    "                      \"PostalCode\", \"City\", \"StateProv\", \"Country\", \"Phone1\", \"Phone2\", \"Phone3\",\n",
    "                      \"Email1\", \"Email2\", \"NationalTaxRateDesc\", \"NationalTaxRate\", \"LocalTaxRateDesc\",\n",
    "                      \"LocalTaxRate\", \"AgencyID\", \"CreditRating\", \"NetWorth\", \"MarketingNameplate\", \n",
    "                      \"IsCurrent\", \"BatchID\", \"EffectiveDate\", \"EndDate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts = ET.parse('Dataset/CustomerMgmt.xml').iter(\"Customer\")    \n",
    "\n",
    "Dim_account_list= []\n",
    "sk = 0\n",
    "for i, x in enumerate(accounts, 0):\n",
    "    \n",
    "    if action_type_for_customers[i] == \"NEW\":\n",
    "        # Here: We extract all data we need from xml \n",
    "        C_ID = x.attrib[\"C_ID\"]\n",
    "        C_TAX_ID = x.attrib[\"C_TAX_ID\"]\n",
    "        C_GNDR = x.attrib[\"C_GNDR\"]\n",
    "        Tier = x.attrib[\"C_GNDR\"]\n",
    "        LastName = x.find(\"Name/C_F_NAME\").text\n",
    "        FirstName = x.find(\"Name/C_F_NAME\").text\n",
    "        MiddleInitial = x.find(\"Name/MiddleInitial\").text if x.find(\"Name/MiddleInitial\") else None\n",
    "        CA_B_ID = x.find(\"Name/C_F_NAME\").text\n",
    "        CA_NAME = x.find(\"Account/CA_NAME\").text\n",
    "        Dim_account_list.append((sk, CA_ID, CA_B_ID, C_ID, \"Active\", CA_NAME, CA_TAX_ST, True, \n",
    "               1, datetime.now(), \"9999-12-31\"))\n",
    "        \n",
    "    elif action_type_for_customers[i] == \"UPDACCT\":\n",
    "        C_ID = x.attrib[\"C_ID\"]\n",
    "        CA_ID = x.find(\"Account\").attrib[\"CA_ID\"]\n",
    "        CA_TAX_ST = x.find(\"Account\").attrib[\"CA_TAX_ST\"] if \"CA_TAX_ST\" in x.find(\"Account\").attrib else None\n",
    "        \n",
    "        # check if part is exist if not set it to None.\n",
    "        CA_B_ID = x.find(\"Account/CA_B_ID\").text if x.find(\"Account/CA_B_ID\") != None else None\n",
    "        CA_NAME = x.find(\"Account/CA_NAME\").text if x.find(\"Account/CA_NAME\") != None else None\n",
    "        new_values = (sk, CA_ID, CA_B_ID, C_ID, \"Active\", CA_NAME, CA_TAX_ST, True, \n",
    "               1, datetime.now(), \"9999-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50092, '64', '34955', '64', 'Active', None, '1', True, 1, datetime.datetime(2020, 4, 4, 17, 30, 51, 267878), '9999-12-31')\n",
      "(50149, '293', '45669', '11', 'Active', None, '1', True, 1, datetime.datetime(2020, 4, 4, 17, 34, 50, 956259), '9999-12-31')\n",
      "(50152, '286', '33966', '110', 'Active', None, '0', True, 1, datetime.datetime(2020, 4, 4, 17, 35, 10, 359756), '9999-12-31')\n",
      "(50154, '220', '15766', '114', 'Active', None, '1', True, 1, datetime.datetime(2020, 4, 4, 17, 35, 24, 611794), '9999-12-31')\n",
      "(50166, '197', '38615', '123', 'Active', None, '1', True, 1, datetime.datetime(2020, 4, 4, 17, 36, 58, 866095), '9999-12-31')\n",
      "(50175, '133', '28543', '31', 'Active', None, '0', True, 1, datetime.datetime(2020, 4, 4, 17, 38, 18, 340420), '9999-12-31')\n",
      "(50191, '70', '13655', '70', 'Active', None, '1', True, 1, datetime.datetime(2020, 4, 4, 17, 41, 2, 525894), '9999-12-31')\n",
      "(50205, '68', '46951', '68', 'Active', None, '2', True, 1, datetime.datetime(2020, 4, 4, 17, 43, 49, 18488), '9999-12-31')\n",
      "(50219, '469', '1543', '294', 'Active', None, '1', True, 1, datetime.datetime(2020, 4, 4, 17, 46, 50, 150527), '9999-12-31')\n",
      "(50232, '332', '46951', '227', 'Active', None, '1', True, 1, datetime.datetime(2020, 4, 4, 17, 49, 58, 427347), '9999-12-31')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-087203d90a32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mmatched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDimAccount_new_addacc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDimAccount_new_addacc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAccountID\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCA_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         full_updated_values = tuple([update if update != \"None\" else old \n\u001b[1;32m     16\u001b[0m                                      for update, old in zip(updated_account_values, matched)])  \n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \"\"\"\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's handle update action\n",
    "accounts = ET.parse('Dataset/CustomerMgmt.xml').iter(\"Customer\")    \n",
    "for i, x in enumerate(accounts, 0):\n",
    "    if action_type_for_customers[i] == \"UPDACCT\":\n",
    "        C_ID = x.attrib[\"C_ID\"]\n",
    "        CA_ID = x.find(\"Account\").attrib[\"CA_ID\"]\n",
    "        CA_TAX_ST = x.find(\"Account\").attrib[\"CA_TAX_ST\"] if \"CA_TAX_ST\" in x.find(\"Account\").attrib else \"None\"\n",
    "        CA_B_ID = x.find(\"Account/CA_B_ID\").text if x.find(\"Account/CA_B_ID\") != None else None\n",
    "        CA_NAME = x.find(\"Account/CA_NAME\").text if x.find(\"Account/CA_NAME\") != None else None\n",
    "        updated_account_values = (sk, CA_ID, CA_B_ID, C_ID, \"Active\", CA_NAME, CA_TAX_ST, True, \n",
    "               1, datetime.now(), \"9999-12-31\")\n",
    "        \n",
    "        \n",
    "        matched = DimAccount_new_addacc.filter(DimAccount_new_addacc.AccountID == CA_ID).collect()[-1]  \n",
    "        full_updated_values = tuple([update if update != \"None\" else old \n",
    "                                     for update, old in zip(updated_account_values, matched)])  \n",
    "        try:\n",
    "            \n",
    "            new_record = sparkSession.createDataFrame([full_updated_values,], columns_name)\n",
    "        except:\n",
    "            print(full_updated_values)\n",
    "        DimAccount_new_addacc = DimAccount_new_addacc.union(new_record)\n",
    "        sk +=1\n",
    "        \n",
    "DimAccount_new_addacc.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_elementtree._element_iterator at 0x7f8fc86a2870>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## الشغل الي تحت عك "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load status table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's join Account table with status table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_with_status = account_df.join(\n",
    "                        broadcast(status_df), \n",
    "                        account_df.CA_ST_ID == status_df.ST_ID,   \n",
    "                        'inner'\n",
    "                  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----+--------+--------+--------------------+---------+--------+-----+--------+\n",
      "|CDC_FLAG|CDC_DSN|CA_ID|CA_B_ID |CA_C_ID |             CA_NAME|CA_TAX_ST|CA_ST_ID|ST_ID| ST_NAME|\n",
      "+--------+-------+-----+--------+--------+--------------------+---------+--------+-----+--------+\n",
      "|       I|  43490|30470|   16206|   15280|XkRcJWPVFFSGAtTGo...|        1|    ACTV| ACTV|  Active|\n",
      "|       U|  43491|13857|   35351|    4996|kXUQTTuZHQsJsIDcB...|        1|    ACTV| ACTV|  Active|\n",
      "|       U|  43492|26685|   23304|    2762|ruXPPxRMDLjswZZHv...|        1|    INAC| INAC|Inactive|\n",
      "|       I|  43493|30471|   43026|   15281|arQHNWBBCOGMxvWqT...|        2|    ACTV| ACTV|  Active|\n",
      "|       I|  43494|30472|    5711|   15282|DuQgzgldMMnEnh Fh...|        1|    ACTV| ACTV|  Active|\n",
      "+--------+-------+-----+--------+--------+--------------------+---------+--------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "account_with_status.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load customer.xml file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a generator of dict containing account data \n",
    "accounts_dict = ET.parse('Dataset/CustomerMgmt.xml').iter(\"@ActionType\")    \n",
    "c_ids = []\n",
    "for x in accounts_dict:\n",
    "    print(x)\n",
    "    if x.find(\"Account\") != None:\n",
    "        c_ids.append(x.attrib[\"C_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Dataframe \n",
    "customer_pd = pd.DataFrame(customer_data_array, \n",
    "                 columns=[\"C_ID\", \"ACTION_TYPE\", \"CA_ID\"], index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----+\n",
      "|C_ID|ACTION_TYPE|CA_ID|\n",
      "+----+-----------+-----+\n",
      "|   0|        NEW|    0|\n",
      "|   1|        NEW|    1|\n",
      "|   2|        NEW|    2|\n",
      "|   3|        NEW|    3|\n",
      "|   4|        NEW|    4|\n",
      "+----+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mySchema = StructType([ StructField(\"C_ID\", StringType(), True)\\\n",
    "\n",
    "                       ,StructField(\"ACTION_TYPE\", StringType(), True)\\\n",
    "\n",
    "                       ,StructField(\"CA_ID\", StringType(), True)])\n",
    "customer_df = sparkSession.createDataFrame(customer_pd, schema = mySchema)\n",
    "\n",
    "customer_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join account_with_status with customer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_account_df = account_with_status.join(\n",
    "                        customer_df, \n",
    "                        account_with_status.CA_ID == customer_df.CA_ID,   \n",
    "                        'inner'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----+--------+--------+--------------------+---------+--------+-----+--------+----+-----------+-----+\n",
      "|CDC_FLAG|CDC_DSN|CA_ID|CA_B_ID |CA_C_ID |             CA_NAME|CA_TAX_ST|CA_ST_ID|ST_ID| ST_NAME|C_ID|ACTION_TYPE|CA_ID|\n",
      "+--------+-------+-----+--------+--------+--------------------+---------+--------+-----+--------+----+-----------+-----+\n",
      "|       U|  43579|  751|   36480|     436|SBkXmBJLgAbOmSROj...|        1|    INAC| INAC|Inactive| 436|        NEW|  751|\n",
      "|       U|  43574| 1143|    1474|     618|INkSQXOCuakseRkSa...|        2|    ACTV| ACTV|  Active| 618|        NEW| 1143|\n",
      "|       U|  43561| 3568|   30347|    1761|LgEiiaOJQMRJNcDMm...|        1|    ACTV| ACTV|  Active|1761|    ADDACCT| 3568|\n",
      "|       U|  43561| 3568|   30347|    1761|LgEiiaOJQMRJNcDMm...|        1|    ACTV| ACTV|  Active|1761|    UPDACCT| 3568|\n",
      "|       U|  43553| 4128|   28792|    1607|FdnzvlBxEzFnsRpVd...|        1|    INAC| INAC|Inactive|1607|    ADDACCT| 4128|\n",
      "+--------+-------+-----+--------+--------+--------------------+---------+--------+-----+--------+----+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_account_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CDC_FLAG',\n",
       " 'CDC_DSN',\n",
       " 'CA_B_ID ',\n",
       " 'CA_C_ID ',\n",
       " 'CA_NAME',\n",
       " 'CA_TAX_ST',\n",
       " 'ST_NAME',\n",
       " 'C_ID',\n",
       " 'ACTION_TYPE']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_account_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BinaryType – Binary data.\n",
    "BooleanType – Boolean values.\n",
    "ByteType – A byte value.\n",
    "DateType – A datetime value.\n",
    "DoubleType – A floating-point double value.\n",
    "IntegerType – An integer value.\n",
    "LongType – A long integer value.\n",
    "NullType – A null value.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
